{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOQMS54kEkrq"
   },
   "source": [
    "# 데이터사이언스 (0010085001)\n",
    "\n",
    "## Exercise 12: Linear Regression\n",
    "\n",
    "In this excercise, we will implement the linear regression algorithm.\n",
    "\n",
    "- Regression models are used to describe relationship between variables by fitting a line to the observed data. \n",
    "- Regression allows you to desimtate how a dependent variable changes as the independent variable(s) change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGURm8ewQI-_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8AQY_tZQQRo"
   },
   "outputs": [],
   "source": [
    "# 1. 데이터 생성\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "\n",
    "data = \n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TfcDSP4x2AC"
   },
   "outputs": [],
   "source": [
    "# regression 에 필요한 데이터만 선택\n",
    "data = \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p-s6V-vM0i9"
   },
   "source": [
    "### 1. Simple Linear Regression\n",
    "\n",
    "Simple Linear Regression is used to estimate the relationship between two quantitative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_pWOtKoGCxE"
   },
   "outputs": [],
   "source": [
    "# 2. 데이터 분리\n",
    "\n",
    "# 2.1 inputs (X) 설정\n",
    "X = \n",
    "print(X.shape)\n",
    "\n",
    "# 차원 확장 (150, ) -> (150, 1)\n",
    "X = \n",
    "\n",
    "# bias 추가 (linear classification 과 동일)\n",
    "oneVector = \n",
    "print(oneVector.shape)\n",
    "\n",
    "# bias 와 기존의 X 를 결합하기\n",
    "X = \n",
    "\n",
    "# 주어진 입력을 matrix 형태로 변경\n",
    "X = \n",
    "print('inputs: ', X.shape)\n",
    "\n",
    "# 2.2 targets (Y) 설정\n",
    "Y = \n",
    "print('targets: ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwG27f45EsY6"
   },
   "outputs": [],
   "source": [
    "# 3. linear regression 수행\n",
    "\n",
    "# X, Y 를 관계짓는 weights ([w0, w1]) 을 찾기 위해서는 pseudo-inverse 계산이 필요함\n",
    "# pseudo-inverse 는 ((X_t*X)^-1)*X^T 를 통해 이루어짐\n",
    "\n",
    "# X_t 계산\n",
    "XT = \n",
    "\n",
    "# (X_t * X) 계산\n",
    "XTX = \n",
    "\n",
    "# (X_t * X)^-1 계산\n",
    "XTXINV = \n",
    "\n",
    "# ((X_t*X)^-1)*X^T -> pseudo-inverse\n",
    "XTXINVTR = \n",
    "\n",
    "# targets (Y) 와의 inner product 를 통해서 closed-form solution 을 구할 수 있음\n",
    "w_lin = np.dot(XTXINVTR, Y)\n",
    "print(w_lin)\n",
    "print(w_lin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf6ns3yPJ5BP"
   },
   "outputs": [],
   "source": [
    "# 4. 시각화\n",
    "\n",
    "intercept = \n",
    "slope = \n",
    "\n",
    "X_ = np.squeeze(np.asarray(X[:, 1]))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_, Y, c=Y, cmap=plt.cm.Spectral)\n",
    "\n",
    "# line 생성\n",
    "x_hyperplane = np.linspace(4, 8, 10)\n",
    "y_hyperplane = \n",
    "\n",
    "plt.plot(x_hyperplane, y_hyperplane, '-')\n",
    "plt.title(\"Simple linear regression algorithm on Iris dataset\")\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"x_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrO-P3oS41vd"
   },
   "outputs": [],
   "source": [
    "# 5. error 계산\n",
    "\n",
    "# w_lin 과 입력의 inner-product 를 통해 우리의 예측 계산\n",
    "Y_pred = \n",
    "\n",
    "# Y로부터 차원 확장으로 Y_true 계산\n",
    "Y_true = \n",
    "\n",
    "# Y_pred 와 Y_true 의 residual 계산\n",
    "residual = \n",
    "\n",
    "errors_l2 = \n",
    "print('Sum of squared errors is: ', errors_l2)\n",
    "\n",
    "errors_l1 = \n",
    "print('Sum of absolute errors is: ', errors_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRFNSVH1M67D"
   },
   "source": [
    "### 2. Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable.\n",
    "predic 변수가 2개 이상인 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "272KgjloM6dR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. 데이터 분리\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 2.1 inputs (X) 설정\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;66;03m# 2개의 칼럼\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# bias 추가 (linear classification 과 동일)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. 데이터 분리\n",
    "\n",
    "# 2.1 inputs (X) 설정\n",
    "X = data[:, 0:2] # 2개의 칼럼\n",
    "print(X.shape)\n",
    "\n",
    "# bias 추가 (linear classification 과 동일)\n",
    "oneVector = np.ones((X.shape[0],1))\n",
    "\n",
    "# bias 와 기존의 X 를 결합하기\n",
    "X = np.concatenate((oneVector, X), axis=1)\n",
    "\n",
    "# 주어진 입력을 matrix 형태로 변경\n",
    "X = np.asmatrix(X)\n",
    "print('inputs: ', X.shape)\n",
    "\n",
    "# 2.2 targets (Y) 설정\n",
    "Y = data[:, 3]\n",
    "print('targets: ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53RC3BtyHj9z"
   },
   "outputs": [],
   "source": [
    "# 3. linear regression 수행\n",
    "\n",
    "# X, Y 를 관계짓는 weights ([w0, w1, w2]) 을 찾기 위해서는 pseudo-inverse 계산이 필요함\n",
    "# pseudo-inverse 는 ((X_t*X)^-1)*X^T 를 통해 이루어짐\n",
    "\n",
    "# X_t 계산\n",
    "XT = \n",
    "\n",
    "# (X_t * X) 계산\n",
    "XTX = \n",
    "\n",
    "# (X_t * X)^-1 계산\n",
    "XTXINV = \n",
    "\n",
    "# ((X_t*X)^-1)*X^T -> pseudo-inverse\n",
    "XTXINVTR = \n",
    "\n",
    "# targets (Y) 와의 inner product 를 통해서 closed-form solution 을 구할 수 있음\n",
    "w_lin = \n",
    "print(w_lin)\n",
    "print(w_lin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvuPG8qs7pY7"
   },
   "outputs": [],
   "source": [
    "# 4. error 계산\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# w_lin 과 입력의 inner-product 를 통해 우리의 예측 계산\n",
    "Y_pred = \n",
    "\n",
    "# Y로부터 차원 확장으로 Y_true 계산\n",
    "Y_true = \n",
    "\n",
    "# Y_pred 와 Y_true 의 residual 계산\n",
    "residual = \n",
    "\n",
    "print('Erros for {}-th fold'.format(k))\n",
    "errors_l2 = LA.norm((Y_pred - Y_true), ord=2)\n",
    "print('Sum of squared errors (MSE) is: ', errors_l2)\n",
    "\n",
    "errors_l1 = LA.norm((Y_pred - Y_true), ord=1)\n",
    "print('Sum of absolute errors (MAE) is: ', errors_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kNp0uokyMeH"
   },
   "source": [
    "### 3. K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pZA2k982xm24"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# k-fold는 데이터를 k등분함\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mload_iris()\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtarget\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# k-fold는 데이터를 k등분함\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "# 교차검증 횟수만큼 for loop 수행\n",
    "for k in range (0,10):\n",
    "    # 학습 (training) / 평가 (test) 데이터 구분\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    X_train, X_test, _, _ = train_test_split(X, Y, test_size=0.2, random_state=k)\n",
    "    \n",
    "    # targets (Y) 설정\n",
    "    Y_train = X_train[:, 3]\n",
    "    Y_test = X_test[:, 3]\n",
    "\n",
    "    # inputs (X) 설정\n",
    "    X_train = X_train[:, 0]\n",
    "    X_test = X_test[:, 0]\n",
    "    \n",
    "    # 차원 확장 \n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "    # bias 추가 (linear classification 과 동일)\n",
    "    oneVector_train = np.ones((X_train.shape[0], 1))\n",
    "    oneVector_test = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "    # bias 와 기존의 X_train 를 결합하기\n",
    "    X_train = np.concatenate((oneVector_train, X_train), axis=1)\n",
    "    X_test = np.concatenate((oneVector_test, X_test), axis=1)\n",
    "\n",
    "    ######################\n",
    "    ### training phase ###\n",
    "    ######################\n",
    "\n",
    "    # 주어진 입력을 matrix 형태로 변경\n",
    "    X_train = np.asmatrix(X_train)\n",
    "    \n",
    "    # Training the model using Linear Regresion \n",
    "    XT = np.transpose( X_train )\n",
    "    XTX = np.matmul( XT, X_train )\n",
    "    XTXINV = np.linalg.inv( XTX )\n",
    "    XTXINVTR = np.dot( XTXINV, XT )\n",
    "    \n",
    "    # k 번째 검증에 대한 w_lin 계산\n",
    "    w_lin = np.dot( XTXINVTR, Y_train ) \n",
    "\n",
    "    w_lin_ = np.transpose(w_lin)\n",
    "  \n",
    "  \n",
    "    ######################\n",
    "    ##### test phase #####\n",
    "    ######################\n",
    "\n",
    "    # w_lin 과 X_test의 inner-product 를 통해 우리의 예측 계산\n",
    "    Y_pred = np.matmul(X_test, w_lin_)\n",
    "\n",
    "    # Y로부터 차원 확장으로 Y_true 계산\n",
    "    Y_true = np.expand_dims(Y_test, axis=1) \n",
    "\n",
    "    # Y_pred 와 Y_true 의 residual 계산\n",
    "    residual = Y_pred - Y_test\n",
    "\n",
    "    print('Erros for {}-th fold'.format(k))\n",
    "    errors_l2 = LA.norm((Y_pred - Y_true), ord=2)\n",
    "    print('Sum of squared errors (MSE) is: ', errors_l2)\n",
    "\n",
    "    errors_l1 = LA.norm((Y_pred - Y_true), ord=1)\n",
    "    print('Sum of absolute errors (MAE) is: ', errors_l1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
