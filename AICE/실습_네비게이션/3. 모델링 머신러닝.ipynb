{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f044ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "데이터 불러오기부터\n",
    "가. 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 나. 데이터 불러오기\n",
    "df_feature = pd.read_csv(\"onenavi_train_feature.csv\",sep=\"|\")\n",
    "df_target = pd.read_csv(\"onenavi_train_target.csv\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Train/Test Data Split\n",
    "Overfitting을 방지하기위해 우리는 데이터 셋을 분할합니다.\n",
    "KeyPoint : 학습 대상으로 정제한 데이터를 Train/Test 데이터로 분할할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split : 테스트 데이터 20%\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6913a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd46298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[train_test_split Option 설명]\n",
    "\n",
    "test_size : 전체에서 test 데이터 비율을 주는 옵션입니다.(default 0.25)\n",
    "shuffle : 분할 이전에 섞을지 여부를 정해주는 옵션입니다.(default True)\n",
    "stratify : target으로 지정해주면 각 class 비율을 train/test에 적절하게 분배해줍니다. 즉, 쏠림 방지가 가능합니다.(default None)\n",
    "random_state : 분할하는 데이터를 섞을 때 기준(?)이 되어주는 값으로 모델 최적화(튜닝)를 할때는 이 값을 고정시켜 두어야 데이터셋이 변경되지 않아 정확한 비교가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2. Modeling\n",
    "이제 모델링을 해봅시다!\n",
    "KeyPoint : 머신러닝 라이브러리를 토대로 모델링을 할 수 있다.\n",
    "[공식 Document]\n",
    "\n",
    "sklearn(https://scikit-learn.org/stable/user_guide.html)\n",
    "sklearn.linear_model.LinearRegression(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "sklearn.ensemble.RandomForestRegressor(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "sklearn.ensemble.GradientBoostingRegressor(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "xgboost(https://xgboost.readthedocs.io/en/latest/)\n",
    "[여기서 짚고 넘어갈 것!]\n",
    "\n",
    "모델의 평가 기준은 RMSE와 R-squared Score로 한다.(다른 평가 기준도 있으나, 본 실습에서는 두 가지로 진행)\n",
    "RMSE(Root Mean Squared Error) : 그대로 직역하면 Error(예측 값-실제 값) 제곱의 평균에 루트한 값으로 모델의 예측 값과 실제 값의 차이를 표현\n",
    "R-squared Score : 우리말로 결정계수라고 할 수 있는데, 모델의 설명력을 이야기해주는 지표이다.1에 가까울수록 모델의 설명력이 높다고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155be6d0",
   "metadata": {},
   "source": [
    "### 가. LinearRegression 부터 시작해봅시다.\n",
    "#### 통계와 기계학습에서 사용되는 전통적인 기법\n",
    "#### 단순하게(조금은 거칠게) 표현하면  모델이 만들어낸 선형방정식과 실제 값의 오차를 기준으로 제일 작은 산식을 도출하는 방법이라고 할 수 있다.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계기법에서 LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "results = sm.OLS(train_y, train_x).fit()\n",
    "\n",
    "results.summary()\n",
    "\n",
    "# *** p<0.001, ** p<0.01, * p<0.05\n",
    "# https://stats.stackovernet.xyz/ko/q/37406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3db296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계학습에서 LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "model=lr()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "print(\"모델의 회귀계수는 : \", model.coef_, \"이고 모델의 절편은 : \",model.intercept_)\n",
    "\n",
    "pred_y = model.predict(test_x)\n",
    "print(\"RMSE on Test set : {0:.5f}\".format(mean_squared_error(test_y,pred_y)**0.5))\n",
    "print(\"R-squared Score on Test set : {0:.5f}\".format(r2_score(test_y,pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "나. 다음은 앙상블 기법을 토대로 랜덤포레스트와 그라디언부스팅, XG부스팅을 활용해봅시다.\n",
    "본격적인 활용 이전에 앙상블 기법이 무엇인지 잠깐만 살펴보고 진행하겠습니다.\n",
    "앙상블은 우리가 흔히 알고 있는 음악에서 사용되는 용어의 의미와 비슷하게 여러 모델을 이용해서 조화시키는 것으로 설명할 수 있습니다.\n",
    "이를 통해 여러 모델의 장점을 모아 전반적인 오류를 줄여주고 각 모델의 한계를 극복할 수 있습니다.\n",
    "앙상블은 크게 Bagging과 Boosting으로 구분해서 볼 수 있습니다.\n",
    "Bagging : Bootstrap Aggregating의 줄임말로 그대로 풀어보면 여러 개의 bootstrap을 생성하고 합쳐서(Aggregating) 최종 예측 모델을 산출하는 방법이다. 아래는 Bagging을 활용한 랜덤포레스트를 이해하기 쉽도록 도식화 한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43265132",
   "metadata": {},
   "source": [
    "\n",
    "나. 다음은 앙상블 기법을 토대로 랜덤포레스트와 그라디언부스팅, XG부스팅을 활용해봅시다.\n",
    "#### 본격적인 활용 이전에 앙상블 기법이 무엇인지 잠깐만 살펴보고 진행하겠습니다.\n",
    "* 앙상블은 우리가 흔히 알고 있는 음악에서 사용되는 용어의 의미와 비슷하게 여러 모델을 이용해서 조화시키는 것으로 설명할 수 있습니다.\n",
    "* 이를 통해 여러 모델의 장점을 모아 전반적인 오류를 줄여주고 각 모델의 한계를 극복할 수 있습니다.\n",
    "* 앙상블은 크게 Bagging과 Boosting으로 구분해서 볼 수 있습니다.\n",
    "* Bagging : Bootstrap Aggregating의 줄임말로 그대로 풀어보면 여러 개의 bootstrap을 생성하고 합쳐서(Aggregating) 최종 예측 모델을 산출하는 방법이다. 아래는 Bagging을 활용한 랜덤포레스트를 이해하기 쉽도록 도식화 한 것이다.\n",
    "![image.png](attachment:image.png)\n",
    "* Boosting : Bagging이 만든 여러 개의 bootstrap이 서로 영향을 주지 않는 병렬학습이라면 Boosting은 여러 모델을 순차적으로 학습해서 이전 모델의 결과를 바탕으로 다음 모델을 학습하는 기법이다. 이 과정을 통해 모델은 틀린 부분에 가중치를 부여하는 방식으로 오류를 더 잘 잡아낼 수 있도록 고안된 기법인데, Bagging과의 차이를 거칠게 표현하면 Bagging이 일반적인 모델을 여러개 생성하는데 집중한다면 Boosting은 정답을 찾기 어려운 문제에 집중한다고 할 수 있다. 아래는 부스팅을 이해하기 쉽게 도식화 한 것이다.\n",
    "![image-2.png](attachment:image-2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c557d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1) 렌덤포레스트\n",
    "배깅의 일종으로 의사결정나무(Decision Tree) 여러 개를 모아서 숲을 랜덤으로 구성하고 이를 종합해서 최종 모델을 산출하는 기법이라고 할 수 있다.\n",
    "[주요 하이퍼 파라미터]\n",
    "\n",
    "n_estimators : 결정트리의 갯수를 지정하는 것인데, 많을 수록 좋은 결과 값을 기대할 수 있으나, 시간도 비례해서 증가할 수 있다는 것을 명심해야한다.(default 10)\n",
    "max_depth : 트리의 최대 깊이을 설정하는 것으로 많이 깊어질수록 과적합의 가능성이 높아진다.(default None)\n",
    "min_samples_split : 노드를 나눌 수 있는 최소 데이터 수(default 2)\n",
    "min_samples_leaf : 잎이 될 수 있는 최소 데이터 수(default 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# 다차원 배열을 1차원으로 평평하게 만들어주기!\n",
    "train_y = np.ravel(train_y, order='C')\n",
    "\n",
    "model=rfr(n_estimators=100,max_depth=5,min_samples_split=30,min_samples_leaf=15)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "pred_y = model.predict(test_x)\n",
    "print(\"RMSE on Test set : {0:.5f}\".format(mean_squared_error(test_y,pred_y)**0.5))\n",
    "print(\"R-squared Score on Test set : {0:.5f}\".format(r2_score(test_y,pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95281d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature의 중요도 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rf_importances_values = model.feature_importances_\n",
    "rf_importances = pd.Series(rf_importances_values, index = train_x.columns)\n",
    "rf_top10 = rf_importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothicCoding'\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "sns.barplot(x=rf_top10, y=rf_top10.index,palette = \"RdBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2) GradientBoosting\n",
    "앞선 모델의 에러를 다음 모델의 예측 값으로 활용하면서 가중치 업데이트 하는데 경사하강법(Gradient Descent)를 활용해서 최적 모델을 만드는 기법\n",
    "image.png\n",
    "\n",
    "[주요 하이퍼 파라미터]\n",
    "\n",
    "loss : 경사하강법에서 사용할 손실함수를 지정(default ls(Least Square))\n",
    "learning_rate : 말그대로 학습율이다. 기존의 예측 값에 대해 학습 결과를 얼마나 업데이트(보정)할지 지정(default 0.1)\n",
    "n_estimators : 부스팅 단계의 갯수를 지정하는 것인데, 많을 수록 좋은 결과 값을 기대할 수 있으나, 시간도 비례해서 증가할 수 있다는 것을 명심해야한다.(default 10)\n",
    "max_depth : 트리의 최대 깊이을 설정하는 것으로 많이 깊어질수록 과적합의 가능성이 높아진다.(default None)\n",
    "min_samples_split : 노드를 나눌 수 있는 최소 데이터 수(default 2)\n",
    "min_samples_leaf : 잎이 될 수 있는 최소 데이터 수(default 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor as grb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# 다차원 배열을 1차원으로 평평하게 만들어주기!\n",
    "train_y = np.ravel(train_y, order='C')\n",
    "\n",
    "model=grb(n_estimators=100,learning_rate=0.1,max_depth=5,min_samples_split=30,min_samples_leaf=15)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "pred_y = model.predict(test_x)\n",
    "print(\"RMSE on Test set : {0:.5f}\".format(mean_squared_error(test_y,pred_y)**0.5))\n",
    "print(\"R-squared Score on Test set : {0:.5f}\".format(r2_score(test_y,pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature의 중요도 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "grb_importances_values = model.feature_importances_\n",
    "grb_importances = pd.Series(grb_importances_values, index = train_x.columns)\n",
    "grb_top10 = grb_importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothicCoding'\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "sns.barplot(x=grb_top10, y=grb_top10.index,palette = \"RdBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde50d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3) XGBoost\n",
    "부스팅 모델이 매우 강력한 것은 사실이지만 단점도 존재(느리고, 과적합의 이슈), 그래서 GradientBoosting보다 빠르고 규제를 설정해서 과적합 방지가 가능한 XGBoost가 등장\n",
    "[주요 하이퍼 파라미터]\n",
    "\n",
    "booster : 사용할 부스터(default gbtree)\n",
    "gamma : 가지를 칠 때 필요한 최소손실 감소로 클수록 보수적(가지를 치지 않는다)이라고 할 수 있다.(default 0)\n",
    "eta : 말그대로 학습율이다. 기존의 예측 값에 대해 학습 결과를 얼마나 업데이트(보정)할지 지정(default 0.3)\n",
    "n_estimators : 부스팅 단계의 갯수를 지정하는 것인데, 많을 수록 좋은 결과 값을 기대할 수 있으나, 시간도 비례해서 증가할 수 있다는 것을 명심해야한다.(default 100)\n",
    "max_depth : 트리의 최대 깊이을 설정하는 것으로 많이 깊어질수록 과적합의 가능성이 높아진다.(default 6)\n",
    "reg_lambda : 가중치에 대한 L2 정규화(default 1), 커질수록 오버피팅 방지, 너무 큰 가중치를 줄여줄 수 있음\n",
    "reg_alpha : 가중치에 대한 L1 정규화(default 0), 커질수록 오버피팅 방지, 불필요한 가중치를 0으로 만들어서 무시\n",
    "[참고] https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=ko\n",
    "early_stopping_rounds : 조기 종료 옵션, 지정 횟수 동안 결과가 개선 되지 않으면 조기 종료 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# 다차원 배열을 1차원으로 평평하게 만들어주기!\n",
    "train_y = np.ravel(train_y, order='C')\n",
    "\n",
    "model=xgb(n_estimators=100,gamma=1,eta=0.1,max_depth=5,reg_lambda=5,reg_alpha=5)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "pred_y = model.predict(test_x)\n",
    "print(\"RMSE on Test set : {0:.5f}\".format(mean_squared_error(test_y,pred_y)**0.5))\n",
    "print(\"R-squared Score on Test set : {0:.5f}\".format(r2_score(test_y,pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature의 중요도 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "xgb_importances_values = model.feature_importances_\n",
    "xgb_importances = pd.Series(xgb_importances_values, index = train_x.columns)\n",
    "xgb_top10 = xgb_importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothicCoding'\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "sns.barplot(x=xgb_top10, y=xgb_top10.index,palette = \"RdBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "다. 그럼 여러 개의 모델을 한 번에 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as grb\n",
    "from xgboost import XGBRegressor as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "model_list=[\n",
    "            lr(),\n",
    "            rfr(),\n",
    "            grb(),\n",
    "            xgb()\n",
    "            ]\n",
    "\n",
    "# 다차원 배열을 1차원으로 평평하게 만들어주기!\n",
    "train_y = np.ravel(train_y, order='C')\n",
    "\n",
    "model_rslt = []\n",
    "for i in range(len(model_list)):\n",
    "    start_time = time.process_time()\n",
    "    model = model_list[i]\n",
    "    model.fit(train_x, train_y)\n",
    "    end_time = time.process_time()\n",
    "    joblib.dump(model, '{}_model.pkl'.format(i)) # 모델 저장, sklearn을 통해서 만들어진 모델은 pkl 파일로 저장\n",
    "    print(f\"* {model} 결과 시작\")\n",
    "    print('----  {0:.5f}sec, training complete  ----'.format(end_time-start_time))\n",
    "    pred_y = model.predict(test_x)\n",
    "    model_rslt.append(model)\n",
    "    print(\"RMSE on Test set : {0:.5f}\".format(mean_squared_error(test_y,pred_y)**0.5))\n",
    "    print(\"R-squared Score on Test set : {0:.5f}\".format(r2_score(test_y,pred_y)))\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
